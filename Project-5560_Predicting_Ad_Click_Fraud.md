{"cells":[{"cell_type":"markdown","source":["<a href=\"http://www.calstatela.edu/centers/hipic\"><img align=\"left\" src=\"https://avatars2.githubusercontent.com/u/4156894?v=3&s=100\"><image/>\n</a>\n<img align=\"right\" alt=\"California State University, Los Angeles\" src=\"http://www.calstatela.edu/sites/default/files/groups/California%20State%20University%2C%20Los%20Angeles/master_logo_full_color_horizontal_centered.svg\" style=\"width: 360px;\"/>\n\n# CIS5560 Term Project Tutorial"],"metadata":{}},{"cell_type":"markdown","source":["------\n#### Authors: Hai Anh Le, Neha Gupta, Maria Boldina\n\n#### Instructor: [Jongwook Woo](https://www.linkedin.com/in/jongwook-woo-7081a85)\n\n#### Date: 05/18/2017"],"metadata":{}},{"cell_type":"markdown","source":["### Import Spark SQL and Spark ML Libraries"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import date_format\nimport pyspark.sql.functions as func\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\nfrom pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Load Source Data\nThe data for this exercise is provided as a CSV file containing details of users click. The data includes specific characteristics for each user, as well as a column indicating how many user download the app or not."],"metadata":{}},{"cell_type":"code","source":["train_sampleSchema = StructType([\n  StructField(\"ip\", IntegerType(), False),\n  StructField(\"app\", IntegerType(), False),\n  StructField(\"device\", IntegerType(), False),\n  StructField(\"os\", IntegerType(), False),\n  StructField(\"channel\", IntegerType(), False),\n  StructField(\"clicktime\", TimestampType (), False),\n  StructField(\"attributed\", TimestampType(), False),\n  StructField(\"is_attributed\", IntegerType(), False),\n])\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Read csv file from DBFS (Databricks File Systems)\n\n1. After train_sample_1G.csv file is added to the data of the left frame, create a table using the UI, especially, \"Upload File\"\n2. Click \"Preview Table to view the table\" and Select the option as train_sample_1G.csv has a header as the first row: \"First line is header\"\n3. Change the data type of the table columns as shown in train_sampleSchema of the above cell\n4. When you click on create table button, remember the table name, for example, _train_sample_1G_"],"metadata":{}},{"cell_type":"code","source":["%fs ls /FileStore/tables/train_sample_1G.csv"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Create a dataframe from the table, using Spark SQL"],"metadata":{}},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM train_sample_1G_csv\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Counting the counts of 0's and 1's from is_attributed column to check how many users download the app"],"metadata":{}},{"cell_type":"code","source":["count = df.groupBy('is_attributed').count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Visualization of counts"],"metadata":{}},{"cell_type":"code","source":["display(count)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Uncomment if you would want to use sampling.\ntrain = df #.sampleBy(\"is_attributed\", fractions={0: 0.02, 1: 0.02})"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Prepare the Data\nMost modeling begins with exhaustive exploration and preparation of the data. In this example, the data has been cleaned for you. You will simply select a subset of columns to use as *features* and create a Boolean *label* field named **label** with the value **1** for users who downloaded the app, or **0** for the users who did not download the app."],"metadata":{}},{"cell_type":"markdown","source":["### Building the features"],"metadata":{}},{"cell_type":"markdown","source":["###  Feature -1: Prepare time based feature by extracting day of the week and hour of the day from the click time"],"metadata":{}},{"cell_type":"code","source":["train_with_day_of_week = train.withColumn('day_of_week_number',date_format('click_time', 'u').cast('integer')).withColumn('hour_of_day', date_format('click_time', 'H').cast('integer'))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["###  Feature -2: Prepare feature by grouping clicks by combination of (Ip, Day_of_week_number and Hour)"],"metadata":{}},{"cell_type":"code","source":["grpd_by_ip_day_hr = train_with_day_of_week.groupBy('ip', 'day_of_week_number', 'hour_of_day').agg(func.count(func.lit(1)).alias(\"count_by_ip_day_hour\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["###  Adding Features back to the original dataset"],"metadata":{}},{"cell_type":"code","source":["joined_1 = train_with_day_of_week.join(grpd_by_ip_day_hr, ['ip','day_of_week_number','hour_of_day'], \"leftouter\")\ntrain_with_day_of_week.unpersist()\ngrpd_by_ip_day_hr.unpersist()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["###  Feature -3: Prepare feature by grouping clicks by combination of (Ip, App, Operating System, Day_of_week_number and Hour)"],"metadata":{}},{"cell_type":"code","source":["grpd_by_ip_app_os_day_hr = train_with_day_of_week.groupBy('ip', 'app','os','day_of_week_number', 'hour_of_day').agg(func.count(func.lit(1)).alias(\"count_by_ip_app_os_day_hour\"))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["###  Adding Features back to the original dataset"],"metadata":{}},{"cell_type":"code","source":["joined_2 = joined_1.join(grpd_by_ip_app_os_day_hr, ['ip','app','os','day_of_week_number','hour_of_day'], \"leftouter\")\njoined_1.unpersist()\ngrpd_by_ip_app_os_day_hr.unpersist()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["###  Feature -4 :  Prepare feature by grouping clicks by combination of (App, Day_of_week_number and Hour)"],"metadata":{}},{"cell_type":"code","source":["grpd_by_app_day_hr = train_with_day_of_week.groupBy('app','day_of_week_number', 'hour_of_day').agg(func.count(func.lit(1)).alias(\"grpd_by_app_day_hr\"))\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["###  Adding Features back to the original dataset"],"metadata":{}},{"cell_type":"code","source":["joined_3 = joined_2.join(grpd_by_app_day_hr, ['app','day_of_week_number','hour_of_day'], \"leftouter\")\njoined_2.unpersist()\ngrpd_by_app_day_hr.unpersist()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["###  Feature -5 :  Prepare feature by grouping clicks by combination of (Ip, App, Device and Operating System)"],"metadata":{}},{"cell_type":"code","source":["grpd_by_ip_app_dev_os = train_with_day_of_week.groupBy('ip','app','device', 'os').agg(func.count(func.lit(1)).alias(\"grpd_by_ip_app_dev_os\"))\n"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["###  Adding Features back to the original dataset"],"metadata":{}},{"cell_type":"code","source":["joined_4 = joined_3.join(grpd_by_ip_app_dev_os, ['ip','app','device','os'], \"leftouter\")\njoined_3.unpersist()\ngrpd_by_ip_app_dev_os.unpersist()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["###  Feature -6 :  Prepare feature by grouping clicks by combination of (Ip, Device and Operating System)"],"metadata":{}},{"cell_type":"code","source":["grpd_by_ip_dev_os = train_with_day_of_week.groupBy('ip','device', 'os').agg(func.count(func.lit(1)).alias(\"grpd_by_ip_dev_os\"))\n"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["###  Adding Features back to the original dataset"],"metadata":{}},{"cell_type":"code","source":["joined_5 = joined_4.join(grpd_by_ip_dev_os, ['ip','device','os'], \"leftouter\")\njoined_4.unpersist()\ngrpd_by_ip_dev_os.unpersist()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["###  Consolidating the data and renaming the target column name (is_attributed) to label"],"metadata":{}},{"cell_type":"code","source":["data = joined_5.select('ip', 'device', 'os', 'app', 'day_of_week_number', 'hour_of_day','channel','count_by_ip_day_hour','count_by_ip_app_os_day_hour','grpd_by_app_day_hr','grpd_by_ip_app_dev_os','grpd_by_ip_dev_os', (col(\"is_attributed\").cast(\"Int\").alias(\"label\")))\n"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["### Split the Data\nIt is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 70% of the data for training, and reserve 30% for testing."],"metadata":{}},{"cell_type":"code","source":["splits = data.randomSplit([0.7, 0.3],4272)\ntrainingData = splits[0]\ntrainingData.cache()\nprint trainingData.count() # explicitly calling count to cache the training data in memory\ntestingData = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\nprint testingData.count() # explicitly calling count to cache the testing data in memory\ntestingData.cache()\n"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["### Define the Pipeline\nA predictive model often requires multiple stages of feature preparation. For example, it is common when using some algorithms to distingish between continuous features (which have a calculable numeric value) and categorical features (which are numeric representations of discrete categories). It is also common to *normalize* continuous numeric features to use a common scale (for example, by scaling all numbers to a proportinal decimal value between 0 and 1).\n\nA pipeline consists of a a series of *transformer* and *estimator* stages that typically prepare a DataFrame for\nmodeling and then train a predictive model. In this case, you will create a pipeline with two stages:\n- A **StringIndexer** estimator that converts string values to indexes for categorical features\n- A **VectorAssembler** that combines categorical features into a single vector\n- A **VectorIndexer** that creates indexes for a vector of categorical features\n- A **VectorAssembler** that creates a vector of continuous numeric features\n- A **MinMaxScaler** that normalizes continuous numeric features\n- A **VectorAssembler** that creates a vector of categorical and continuous features\n- A **DecisionTreeClassifier** that trains a classification model."],"metadata":{}},{"cell_type":"code","source":["va = VectorAssembler(inputCols = ['ip', 'device', 'os', 'app', 'day_of_week_number', 'hour_of_day','channel','count_by_ip_day_hour','count_by_ip_app_os_day_hour','grpd_by_app_day_hr','grpd_by_ip_app_dev_os','grpd_by_ip_dev_os'], outputCol=\"features\")\nvi = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["### Assigning pipeline variables for Decision Tree Classifier Model\n\nThe Decision Trees algorithm is popular because it handles categorical data and works out of the box with multiclass classification tasks"],"metadata":{}},{"cell_type":"code","source":["dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxDepth=3)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### Assigning pipeline variables for Random Forest Classifier Model\nRandom Forests uses an ensemble of trees to improve model accuracy. You can read more about Random Forest from the classification and regression section of MLlib Programming Guide."],"metadata":{}},{"cell_type":"code","source":["rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\")"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["### Assigning Pipeline"],"metadata":{}},{"cell_type":"code","source":["dtp = Pipeline(stages=[va, vi, dt])\nrfp = Pipeline(stages=[va, vi, rf])\nmodel = []\n"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### Tune Parameters\nYou can tune parameters to find the best model for your data. A simple way to do this is to use  **TrainValidationSplit** to evaluate each combination of parameters defined in a **ParameterGrid** against a subset of the training data in order to find the best performing parameters.\n\n#### Regularization \nis a way of avoiding Imbalances in the way that the data is trained against the training data so that the model ends up being over fit to the training data. In other words It works really well with the training data but it doesn't generalize well with other data.\nThat we can use a **regularization parameter** to vary the way that the model balances that way.\n\n#### Training ratio of 0.8\nit's going to use 80% of the the data that it's got in its training set to train the model and then the remaining 20% is going to use to validate the trained model. \n\nIn **ParamGridBuilder**, all possible combinations are generated from regParam, maxIter, threshold. So it is going to try each combination of the parameters with 80% of the the data to train the model and 20% to to validate it."],"metadata":{}},{"cell_type":"markdown","source":["###  ParamGridBuilder for Decision Tree Classifier Model"],"metadata":{}},{"cell_type":"code","source":["paramGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n             .addGrid(dt.maxBins, [20, 40, 80])\n             .build())\n"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["###  Building and Training Decision Tree Classifier Model using Train Validation Split"],"metadata":{}},{"cell_type":"code","source":["dt_tvs = TrainValidationSplit(estimator=dtp, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\nmodel.insert(0, dt_tvs.fit(trainingData))\n"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["###  ParamGridBuilder for Random Forest Classifier Model"],"metadata":{}},{"cell_type":"code","source":["paramGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["### Building and Training Random Forest Classifier Model using Train Validation Split"],"metadata":{}},{"cell_type":"code","source":["rf_tvs = TrainValidationSplit(estimator=rfp, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\nmodel.insert(1, rf_tvs.fit(trainingData))"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["### Test the Model\nNow you're ready to use the **transform** method of the model to generate some predictions. You can use this approach to predict if the app will be downloaded where the label is is-attributed. Also in this case you are using the test data which includes a known true label value, so you can compare the predicted number of clicks which actually led to app being downloaded.\n\n\n%md ### Test the Pipeline Model\nThe model produced by the pipeline is a transformer that will apply all of the stages in the pipeline to a specified DataFrame and apply the trained model to generate predictions. In this case, you will transform the **test** DataFrame using the pipeline to generate label predictions."],"metadata":{}},{"cell_type":"code","source":["prediction = [] \npredicted = []\nfor i in range(2):\n  prediction.insert(i, model[i].transform(testingData))\n  predicted.insert(i, prediction[i].select(\"features\", \"prediction\", \"probability\", \"trueLabel\"))\n  predicted[i].show(15)\n"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["### Visualization of Truelable and Prediction for Decision Tree Classifier Model"],"metadata":{}},{"cell_type":"code","source":["display(predicted[0])"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["### Visualization of Truelable and Prediction for Random Forest Classifier Model"],"metadata":{}},{"cell_type":"code","source":["display(predicted[1])"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["### Compute Confusion Matrix Metrics: For Decision Tree Classifier Model and Random Forest Classifier Model\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated."],"metadata":{}},{"cell_type":"code","source":["tp = float(predicted[0].filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted[0].filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted[0].filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted[0].filter(\"prediction == 0.0 AND truelabel == 1\").count())\ndt_metrics = spark.createDataFrame([\n    (\"TP\", tp),\n    (\"FP\", fp),\n    (\"TN\", tn),\n    (\"FN\", fn),\n    (\"Precision\", tp / (tp + fp)),\n    (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\ndt_metrics.show()\n\n"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["### Visualization of Compute Confusion Matrix Metrics of Decision Tree Classifier Model"],"metadata":{}},{"cell_type":"code","source":["display(dt_metrics)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["tp = float(predicted[1].filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted[1].filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted[1].filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted[1].filter(\"prediction == 0.0 AND truelabel == 1\").count())\nrf_metrics = spark.createDataFrame([\n    (\"TP\", tp),\n    (\"FP\", fp),\n    (\"TN\", tn),\n    (\"FN\", fn),\n    (\"Precision\", tp / (tp + fp)),\n    (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nrf_metrics.show()"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["### Visualization of Compute Confusion Matrix Metrics of Random Forest Classifier Model"],"metadata":{}},{"cell_type":"code","source":["display(rf_metrics)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["### Calculating Area Under Curve For Decision Tree Classifier Model"],"metadata":{}},{"cell_type":"code","source":["dtree_evaluator =  BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\ndt_auc = dtree_evaluator.evaluate(prediction[0])\nprint \"AUC for Descision Tree Classifier  \",\" = \", dt_auc"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["### Calculating Area Under Curve For Random Forest Classifier Model"],"metadata":{}},{"cell_type":"code","source":["rf_evaluator =  BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nrf_auc = rf_evaluator.evaluate(prediction[1])\nprint \"AUC for Random Forest Classifier  \",\" = \", rf_auc"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["### Visualization of Area Under Curve For Decision Tree Classifier Model and Random Forest Classifier Model"],"metadata":{}},{"cell_type":"code","source":["display(spark.createDataFrame([ \n  (\"Decision Tree Classifier\", dt_auc), \n  (\"Random Forest Classifier\", rf_auc)], [\"Algorithm\", \"Area Under Curve\"]))"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["### Creating Evaluator for Calculating Root Mean Square Error"],"metadata":{}},{"cell_type":"code","source":["evaluator = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":["### Calculating Root Mean Square Error for Decision Tree Classifier Model"],"metadata":{}},{"cell_type":"code","source":["dtree_rmse = evaluator.evaluate(prediction[0])\nprint \"Root Mean Square Error (RMSE) for Decision Tree Classifier:\", dtree_rmse"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["### Calculating Root Mean Square Error for Random Forest Classifier Model"],"metadata":{}},{"cell_type":"code","source":["rf_rmse = evaluator.evaluate(prediction[1])\nprint \"Root Mean Square Error (RMSE) for Random Forest Classifier:\", rf_rmse"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["### Visualization of Root Mean Square Error for Decision Tree Classifier Model and Random Forest Classifier Model"],"metadata":{}},{"cell_type":"code","source":["display(spark.createDataFrame([ \n  (\"Decision Tree Classifier\", dtree_rmse), \n  (\"Random Forest Classifier\", rf_rmse)], [\"Algorithm\", \"Root Mean Square Error\"]))"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":["### References:\n1. [DataBricks Guide For Spark ML](https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html)"],"metadata":{}}],"metadata":{"name":"Final-Project-5560","notebookId":3030509874578325},"nbformat":4,"nbformat_minor":0}
